{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "714201c3-672b-4151-ac9b-708cf0931c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from causalml.dataset import synthetic_data\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "#from causalml.inference.meta import BaseSClassifier, BaseTClassifier, BaseXClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.base import clone\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "from sklift.metrics import qini_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn import preprocessing\n",
    "import kuplift as kp\n",
    "import warnings\n",
    "import importlib\n",
    "import subprocess\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ray\n",
    "from modèles import *\n",
    "from évaluation import *\n",
    "from generate_synth import *\n",
    "import logging\n",
    "import gc\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "from causalml.dataset import synthetic_data\n",
    "import matplotlib\n",
    "from causalml.inference.meta import BaseSClassifier, BaseTClassifier, BaseXClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import causalml\n",
    "from causalml.inference.tree import CausalTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier\n",
    "from causalml.inference.meta import BaseSClassifier, BaseTClassifier, BaseXClassifier, BaseRClassifier,BaseRRegressor,BaseDRRegressor\n",
    "from causalml.inference.meta import BaseRClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46fdccae-3ee5-418c-94e5-1b4566724f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde72f0-22bc-43f1-94ac-6f8a67921a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   0%|          | 0/150 [00:00<?, ?fichier/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_016.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 16:56:46,432\tINFO worker.py:1770 -- Started a local Ray instance.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-10-03 17:05:46,379 E 2645387 2645387] (raylet) node_manager.cc:3033: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 625d8041f63816bfbf3f58634d24cf8ea02dba3fc16fb6197bda2a18, IP: 10.193.21.24) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.193.21.24`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2024-10-03 17:06:46,381 E 2645387 2645387] (raylet) node_manager.cc:3033: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 625d8041f63816bfbf3f58634d24cf8ea02dba3fc16fb6197bda2a18, IP: 10.193.21.24) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.193.21.24`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(iteration pid=2645569)\u001b[0m /home/nleboudec/miniconda3/envs/python310/lib/python3.10/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 70 leaked semlock objects to clean up at shutdown\n",
      "\u001b[36m(iteration pid=2645569)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(iteration pid=2645569)\u001b[0m /home/nleboudec/miniconda3/envs/python310/lib/python3.10/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 1 leaked folder objects to clean up at shutdown\n",
      "\u001b[36m(iteration pid=2645569)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(iteration pid=2645578)\u001b[0m /home/nleboudec/miniconda3/envs/python310/lib/python3.10/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 70 leaked semlock objects to clean up at shutdown\n",
      "\u001b[36m(iteration pid=2645578)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(iteration pid=2645578)\u001b[0m /home/nleboudec/miniconda3/envs/python310/lib/python3.10/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 1 leaked folder objects to clean up at shutdown\n",
      "Traitement des fichiers CSV:   1%|          | 1/150 [17:50<44:18:23, 1070.49s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_0110.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   1%|▏         | 2/150 [18:30<19:05:03, 464.21s/fichier] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_04068.csv\n"
     ]
    }
   ],
   "source": [
    "dict_rmse_1={'S_learner_XGB' : [], 'T_learner_XGB' : [], 'Baseline average' : [], 'Politique optimale' : []}\n",
    "\n",
    "dict_rmse_2={'S_learner_XGB' : [], 'T_learner_XGB' : [], 'Baseline average' : [], 'Politique optimale' : []}\n",
    "\n",
    "dict_expected={'S_learner_XGB' : [],'T_learner_XGB' : [], 'Baseline average' : [], 'Politique optimale' : []}\n",
    "\n",
    "EXPECTED_S_XGB= {}\n",
    "\n",
    "EXPECTED_T_XGB= {}\n",
    "\n",
    "def get_max_index(array):\n",
    "    # Crée un tableau pour stocker les indices des valeurs maximales\n",
    "    array = np.array(array)\n",
    "    try:\n",
    "        array = array.astype(float)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Le tableau contient des valeurs non numériques qui ne peuvent pas être converties.\")\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    # Parcourir chaque individu (chaque ligne du tableau)\n",
    "    for row in array:\n",
    "        # Vérifie si toutes les valeurs de la ligne sont inférieures ou égales à zéro\n",
    "        if np.all(row <= 0):\n",
    "            result.append(0)  # Si aucune valeur n'est supérieure à 0, ajouter 0\n",
    "        else:\n",
    "            # Sinon, trouve l'indice de la valeur maximale dans la ligne\n",
    "            max_index = np.argmax(row)\n",
    "            result.append(max_index + 1)  # Ajouter +1 pour ne pas avoir 0 comme index\n",
    "            \n",
    "    return np.array(result)\n",
    "\n",
    "@ray.remote\n",
    "class ResultActor:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def update(self, percent, value):\n",
    "        self.data[percent] = value\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class SharedState:\n",
    "    def __init__(self):\n",
    "        self.expected_s_xgb = {}\n",
    "        self.expected_t_xgb = {}\n",
    "        self.EXPECTED_TRUE= {}\n",
    "        self.EXPECTED_random= {}\n",
    "        self.EXPECTED_optimal= {}\n",
    "        self.EXPECTED_zero= {}\n",
    "        self.EXPECTED_average= {}\n",
    "        self.nb_bons_t_S= {}\n",
    "        self.nb_bons_t_T= {}\n",
    "        self.nb_bons_t_true= {}\n",
    "        self.nb_bons_t_random= {}\n",
    "        self.nb_bons_t_average= {}\n",
    "        self.rmse_1_S = {}\n",
    "        self.rmse_1_T = {}\n",
    "        \n",
    "    def update(self, percent, s_value, t_value,expected_true,expected_random,expected_optimal,expected_zero,rmse_1_S,rmse_1_T):\n",
    "        self.expected_s_xgb[percent] = s_value\n",
    "        self.expected_t_xgb[percent] = t_value\n",
    "        self.EXPECTED_TRUE[percent] = expected_true\n",
    "        self.EXPECTED_random[percent] = expected_random\n",
    "        self.EXPECTED_optimal[percent] = expected_optimal\n",
    "        self.EXPECTED_zero[percent] = expected_zero\n",
    "        #self.EXPECTED_average[percent] = expected_average\n",
    "        #self.nb_bons_t_S[percent] = nb_bons_t_S\n",
    "        #self.nb_bons_t_T[percent] = nb_bons_t_T\n",
    "        #self.nb_bons_t_true[percent] = nb_bons_t_true\n",
    "        #self.nb_bons_t_random[percent] = nb_bons_t_random\n",
    "        #self.nb_bons_t_average[percent] = nb_bons_t_average\n",
    "        self.rmse_1_S[percent] = rmse_1_S\n",
    "        self.rmse_1_T[percent] = rmse_1_T\n",
    "\n",
    "    def get_expected_s_xgb(self):\n",
    "        return self.expected_s_xgb\n",
    "\n",
    "    def get_expected_t_xgb(self):\n",
    "        return self.expected_t_xgb\n",
    "\n",
    "    def get_expected_true(self):\n",
    "        return self.EXPECTED_TRUE\n",
    "\n",
    "    def get_expected_random(self):\n",
    "        return self.EXPECTED_random\n",
    "\n",
    "    def get_expected_opti(self):\n",
    "        return self.EXPECTED_optimal\n",
    "\n",
    "    def get_expected_zero(self):\n",
    "        return self.EXPECTED_zero\n",
    "\n",
    "    def get_expected_average(self):\n",
    "        return self.EXPECTED_average\n",
    "\n",
    "    def get_nb_bon_t_S(self):\n",
    "        return self.nb_bons_t_S\n",
    "\n",
    "    def get_nb_bon_t_T(self):\n",
    "        return self.nb_bons_t_T\n",
    "\n",
    "    def get_nb_bon_t_true(self):\n",
    "        return self.nb_bons_t_true\n",
    "\n",
    "    def get_nb_bon_t_random(self):\n",
    "        return self.nb_bons_t_random\n",
    "\n",
    "    def get_nb_bon_t_average(self):\n",
    "        return self.nb_bons_t_average\n",
    "        \n",
    "    def get_rmse_S(self):\n",
    "        return self.rmse_1_S\n",
    "\n",
    "    def get_rmse_T(self):\n",
    "        return self.rmse_1_T\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def iteration(data,percent,shared_state_actor,decoupage=4,nb_per_t=1000,NRA='NRA',valeur_min_int=1,valeur_max_int = 4,nb_inter = 8,X1='X1',X2='X2'):\n",
    "    #data = ray.get(data_ref)\n",
    "\n",
    "    start_time = time.time()\n",
    "   \n",
    "    nombre_valeurs_uniques_T = data['T'].nunique()\n",
    "    df_grand = pd.DataFrame()\n",
    "    seed=42\n",
    "    random.seed(seed)\n",
    "    for i in range(nombre_valeurs_uniques_T):\n",
    "        motif_random = generer_motif_random(valeur_min_int, valeur_max_int, nb_inter)\n",
    "        motif = motif_to_int(motif_random,decoupage=decoupage)\n",
    "        dataset_T = data[data['T'] == i]\n",
    "        #dataset_T[NRA] = 0\n",
    "        dataset_T.loc[:, NRA] = 0\n",
    "        for index, row in dataset_T.iterrows():\n",
    "            for i in motif:\n",
    "                if (i[0][0] <= row[X1] <= i[0][1]) & (i[1][0] <= row[X2] <= i[1][1]):\n",
    "                    #dataset_T.at[index, NRA] = 1\n",
    "                    dataset_T.loc[index, NRA] = 1\n",
    "        data1 = dataset_T.head(nb_per_t)\n",
    "        data1 = drop_nra(data1,k=percent,NRA='NRA')\n",
    "        data2 = dataset_T.iloc[nb_per_t:]\n",
    "        data_NRA_0 = data2[data2['NRA'] == 0]\n",
    "        data_remplace = data_NRA_0.head(int(nb_per_t - len(data1)))\n",
    "        data_percent_NRA = pd.concat([data1, data_remplace], ignore_index=True)\n",
    "        df_grand = pd.concat([df_grand, data_percent_NRA], ignore_index=True)\n",
    "    \n",
    "    del data1, data2, data_NRA_0, data_remplace, data_percent_NRA\n",
    "    gc.collect()\n",
    "\n",
    "    valeurs_unique = df_grand['T'].unique()\n",
    "    \n",
    "    colonnes_a_conserver = ['X1', 'X2', 'T']\n",
    "    X_train = df_grand[colonnes_a_conserver]\n",
    "    colonnes_a_conserver = ['X1', 'X2', 'T', 'Y']\n",
    "    X_train_t = df_grand[colonnes_a_conserver]\n",
    "    colonnes_a_conserver = ['X1', 'X2', 'T']\n",
    "    X_test = data[colonnes_a_conserver]\n",
    "    X_test = X_test.drop(X_train.index)\n",
    "    colonnes_a_conserver = ['Y']\n",
    "    y_train = df_grand[colonnes_a_conserver]\n",
    "    #train_final_filtered = df_grand.loc[:, ['X1', 'X2', 'T', 'Y']]\n",
    "    colonnes_a_conserver = ['X1', 'X2', 'T', 'Y']\n",
    "    X_test_t = data[colonnes_a_conserver]\n",
    "    X_test_t = X_test_t.drop(X_train.index)\n",
    "    test_all = data.copy()\n",
    "    test_all = test_all.drop(X_train.index)\n",
    "    random_uplift = expected_to_uplift(random_expected_outcome(X_test_t, 'T'))\n",
    "    random_policy = uplift_to_policy(random_uplift)\n",
    "    \n",
    "\n",
    "    \n",
    "    X_train = df_grand[['X1', 'X2']]\n",
    "    treatment_train = df_grand['T']\n",
    "    y_train = df_grand['Y']\n",
    "\n",
    "    X_test = data[['X1', 'X2']]\n",
    "    X_test = X_test.drop(X_train.index)\n",
    "    treatment_test = data['T']\n",
    "    treatment_test = treatment_test.drop(X_train.index)\n",
    "    y_test = data['Y']\n",
    "    y_test = y_test.drop(X_train.index)\n",
    "   \n",
    "    data_test_t=data.copy()\n",
    "    column_names = data_test_t.columns.tolist()\n",
    "    data_test_t['policy'] = data_test_t.apply(determine_value, axis=1)\n",
    "    tau_optimal_dict = {}\n",
    "    #colonnes_E_y = [col for col in RMSE_test.columns if col.startswith('E_y|T') and not col.endswith('no_rand')]\n",
    "\n",
    "    #optimal_policy = colonnes_max.apply(lambda x: int(x.split('_')[1]))\n",
    "    optimal_policy = data_test_t['policy']\n",
    "    data_test_t['worst_policy'] = data_test_t.apply(determine_value_2, axis=1)\n",
    "    worst_policy = data_test_t['worst_policy'].to_numpy()\n",
    "    zero_policy=[0]*len(random_policy)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    treatment_train = treatment_train.astype(str)\n",
    "    \n",
    "    #R Learner XGB\n",
    "    \n",
    "    R_L = BaseDRRegressor(XGBRegressor(), control_name='0')\n",
    "\n",
    "    R_L.fit(X=X_train.values, treatment=treatment_train.values, y=y_train.values)\n",
    "\n",
    "\n",
    "    uplift_S_learner_XGB = R_L.predict(X=X_test.values)\n",
    "\n",
    "    uplift_S_learner_XGB = [[1 if x > 1 else -1 if x < -1 else x for x in inner_list] for inner_list in uplift_S_learner_XGB]\n",
    "\n",
    "   \n",
    "    predict_policy_S_learner_XGB = get_max_index(uplift_S_learner_XGB)\n",
    "\n",
    "    #predict_worst_policy_X_learner_XGB = s_learner_model_XGB .predict_worst_policy(X_test)\n",
    "    \n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    start_time = time.time()\n",
    "   \n",
    "    #DR Learner XGB\n",
    "\n",
    "    DR_L = BaseRRegressor(XGBRegressor(), control_name='0')\n",
    "\n",
    "    DR_L.fit(X=X_train.values, treatment=treatment_train.values, y=y_train.values)\n",
    "    \n",
    "    uplift_T_learner_XGB = DR_L.predict(X=X_test.values)\n",
    "\n",
    "    uplift_T_learner_XGB = [[1 if x > 1 else -1 if x < -1 else x for x in inner_list] for inner_list in uplift_T_learner_XGB]\n",
    "\n",
    "    \n",
    "    predict_policy_T_learner_XGB = get_max_index(uplift_T_learner_XGB)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    expected_outcome_S_learner_XGB=expected_outcome(X_test_t,'T','Y',policy=predict_policy_S_learner_XGB)\n",
    "    expected_outcome_T_learner_XGB=expected_outcome(X_test_t,'T','Y',policy=predict_policy_T_learner_XGB)\n",
    "    expected_outcome_true_policy=expected_outcome(X=X_test_t,traitement = 'T',outcome = 'Y')\n",
    "    expected_outcome_random_policy=expected_outcome(X=X_test_t,traitement = 'T',outcome = 'Y',policy=random_policy)\n",
    "    expected_outcome_optimal=expected_outcome(X=X_test_t,traitement = 'T',outcome = 'Y',policy=optimal_policy)\n",
    "    expected_outcome_zero_policy=expected_outcome(X_test_t,'T','Y',policy=zero_policy)\n",
    "\n",
    "    politiques = ['S-Learner_XGB'\n",
    "                'T-Learner_XGB', \n",
    "                'True Policy', 'Random Policy', 'Optimal Policy','Zero policy', 'S_Learner MLP2','Baseline average']\n",
    "\n",
    "    values = [expected_outcome_S_learner_XGB, \n",
    "             expected_outcome_T_learner_XGB,  \n",
    "            expected_outcome_true_policy, expected_outcome_random_policy,expected_outcome_optimal,expected_outcome_zero_policy]\n",
    "\n",
    "    politiques, values = zip(*sorted(zip(politiques, values), key=lambda x: x[1], reverse=False))\n",
    "\n",
    "    bar_width = 0.5\n",
    "    bar_positions = np.arange(len(politiques))\n",
    "    brilliant_color = (1.0, 0.8, 0.0)\n",
    "\n",
    "    tau_1_0 = test_all['tau_1_0'].values\n",
    "    tau_2_0 = test_all['tau_2_0'].values\n",
    "\n",
    "    tau_columns = [col for col in test_all.columns if col.startswith('tau_')]\n",
    "    num_tau_columns = len(tau_columns)\n",
    "    rmse_list = []\n",
    "    for i, tau_col in enumerate(tau_columns):\n",
    "        true_values = test_all[tau_col].values\n",
    "        \n",
    "        pred_value = [sublist[i] for sublist in uplift_S_learner_XGB]\n",
    "        rmse = calculate_rmse(true_values, pred_value)\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "    # Calculer la RMSE moyenne\n",
    "    average_rmse = np.mean(rmse_list)\n",
    "    tau_1_0_S_learner_XGB = [item[0] for item in uplift_S_learner_XGB]\n",
    "    tau_2_0_S_learner_XGB = [item[1] for item in uplift_S_learner_XGB]\n",
    "    rmse_tau_1_0_S_learner_XGB = average_rmse\n",
    "    rmse_tau_2_0_S_learner_XGB = np.sqrt(mean_squared_error(tau_2_0, tau_2_0_S_learner_XGB))\n",
    "\n",
    "    rmse_list = []\n",
    "    for i, tau_col in enumerate(tau_columns):\n",
    "        true_values = test_all[tau_col].values\n",
    "        \n",
    "        pred_value = [sublist[i] for sublist in uplift_T_learner_XGB]\n",
    "        rmse = calculate_rmse(true_values, pred_value)\n",
    "        rmse_list.append(rmse)\n",
    "    average_rmse = np.mean(rmse_list)\n",
    "\n",
    "    tau_1_0_T_learner_XGB = [item[0] for item in uplift_T_learner_XGB]\n",
    "    tau_2_0_T_learner_XGB = [item[1] for item in uplift_T_learner_XGB]\n",
    "    rmse_tau_1_0_T_learner_XGB = average_rmse\n",
    "    rmse_tau_2_0_T_learner_XGB = np.sqrt(mean_squared_error(tau_2_0, tau_2_0_T_learner_XGB))\n",
    "\n",
    "    tau_1_0_optimal = test_all.apply(lambda row: row['E_y|T1_no_rand'] - row['E_y|T0_no_rand'], axis=1)\n",
    "    tau_2_0_optimal = test_all.apply(lambda row: row['E_y|T2_no_rand'] - row['E_y|T0_no_rand'], axis=1)\n",
    "    uplift_optimal = [[diff, value] for diff, value in zip(tau_1_0_optimal, tau_2_0_optimal)]\n",
    "    rmse_tau_1_0_optimal = np.sqrt(mean_squared_error(tau_1_0, tau_1_0_optimal))\n",
    "    rmse_tau_2_0_optimal = np.sqrt(mean_squared_error(tau_2_0, tau_2_0_optimal))\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    \n",
    "    methods = ['S_learner_XGB', 'T_learner_XGB', 'Optimal','Baseline average']\n",
    "\n",
    "    rmse_tau_1_0_list = [rmse_tau_1_0_S_learner_XGB, rmse_tau_1_0_T_learner_XGB, rmse_tau_1_0_optimal]\n",
    "\n",
    "    rmse_tau_2_0_list = [rmse_tau_2_0_S_learner_XGB, rmse_tau_2_0_T_learner_XGB, rmse_tau_2_0_optimal]\n",
    "\n",
    "    pairs_tau_1_0 = zip(methods, rmse_tau_1_0_list)\n",
    "    pairs_tau_2_0 = zip(methods, rmse_tau_2_0_list)\n",
    "    sorted_pairs_tau_1_0 = sorted(pairs_tau_1_0, key=lambda x: x[1])\n",
    "    sorted_pairs_tau_2_0 = sorted(pairs_tau_2_0, key=lambda x: x[1])\n",
    "    sorted_methods_tau_1_0, sorted_rmse_tau_1_0 = zip(*sorted_pairs_tau_1_0)\n",
    "    sorted_methods_tau_2_0, sorted_rmse_tau_2_0 = zip(*sorted_pairs_tau_2_0)\n",
    "    OPTI = optimal_policy[:1000]\n",
    "    \n",
    "    optimal_policy_df = pd.DataFrame(OPTI, columns=['policy'])\n",
    "    optimal_policy_random = optimal_policy_df.sample(n=100, random_state=42)\n",
    "\n",
    "    true_expected_outcome_S_learner_XGB = true_expected_outcome(test_all,predict_policy_S_learner_XGB)\n",
    "    true_expected_outcome_T_learner_XGB = true_expected_outcome(test_all,predict_policy_T_learner_XGB)\n",
    "    true_expected_outcome_optimal = true_expected_outcome(test_all,optimal_policy)\n",
    "    \n",
    "    dict_rmse_1['S_learner_XGB'].append(rmse_tau_1_0_S_learner_XGB)\n",
    "    dict_rmse_1['T_learner_XGB'].append(rmse_tau_1_0_T_learner_XGB)\n",
    "    dict_rmse_1['Politique optimale'].append(rmse_tau_1_0_optimal)\n",
    "    dict_rmse_2['S_learner_XGB'].append(rmse_tau_2_0_S_learner_XGB)\n",
    "    dict_rmse_2['T_learner_XGB'].append(rmse_tau_2_0_T_learner_XGB)\n",
    "    dict_rmse_2['Politique optimale'].append(rmse_tau_2_0_optimal)\n",
    "    dict_expected['S_learner_XGB'].append(expected_outcome_S_learner_XGB)\n",
    "    dict_expected['T_learner_XGB'].append(expected_outcome_T_learner_XGB)\n",
    "    dict_expected['Politique optimale'].append(expected_outcome_optimal)\n",
    "    X1 = X_train_t[X_train_t['T'] == 1]['X1']\n",
    "    X2 = X_train_t[X_train_t['T'] == 1]['X2']\n",
    "    E_y_T0 = X_train_t[X_train_t['T'] == 1]['Y']\n",
    "    EXPECTED_S_XGB = {percent: true_expected_outcome_S_learner_XGB}\n",
    "    EXPECTED_T_XGB = {percent: true_expected_outcome_T_learner_XGB}\n",
    "    EXPECTED_TRUE = {percent: expected_outcome_true_policy}\n",
    "    EXPECTED_random = {percent: expected_outcome_random_policy}\n",
    "    EXPECTED_optimal = {percent: true_expected_outcome_optimal}\n",
    "    EXPECTED_zero = {percent: expected_outcome_zero_policy}\n",
    "    \n",
    "    rmse_S = {percent: rmse_tau_1_0_S_learner_XGB}\n",
    "    rmse_T = {percent: rmse_tau_1_0_T_learner_XGB}\n",
    "    \n",
    "    shared_state_actor.update.remote(percent, EXPECTED_S_XGB[percent], EXPECTED_T_XGB[percent],EXPECTED_TRUE[percent],EXPECTED_random[percent],EXPECTED_optimal[percent],\n",
    "                                     EXPECTED_zero[percent],rmse_S[percent],rmse_T[percent])\n",
    "    ray.get(result_actor.update.remote(percent, expected_outcome_S_learner_XGB))\n",
    "    return expected_outcome_S_learner_XGB, expected_outcome_T_learner_XGB\n",
    "\n",
    "dataset_directory = A DEFINIR\n",
    "\n",
    "for filename in tqdm(os.listdir(dataset_directory), desc=\"Traitement des fichiers CSV\", unit=\"fichier\"):\n",
    "    #if filename.endswith(\".csv\") and filename not in exclusions:\n",
    "    if filename.endswith(\".csv\"):  # Vérifie si le fichier est un fichier CSV\n",
    "        print(filename)\n",
    "        result_actor = ResultActor.remote()\n",
    "        shared_state = SharedState.remote()\n",
    "        file_path = os.path.join(dataset_directory, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        percentages = list(range(0, 101, 5))\n",
    "        futures = [iteration.remote(data,i,shared_state,nb_per_t=5000) for i in percentages]\n",
    "        results = ray.get(futures)\n",
    "        DICT8_EXPECTED = ray.get(result_actor.get_data.remote())\n",
    "        EXPECTED_S_XGB_total = ray.get(shared_state.get_expected_s_xgb.remote())\n",
    "        EXPECTED_T_XGB_total = ray.get(shared_state.get_expected_t_xgb.remote())\n",
    "        EXPECTED_TRUE = ray.get(shared_state.get_expected_true.remote())\n",
    "        EXPECTED_random = ray.get(shared_state.get_expected_random.remote())\n",
    "        EXPECTED_optimal = ray.get(shared_state.get_expected_opti.remote())\n",
    "        EXPECTED_zero = ray.get(shared_state.get_expected_zero.remote())\n",
    "        \n",
    "        \n",
    "        rmse_S = ray.get(shared_state.get_rmse_S.remote())\n",
    "        rmse_T = ray.get(shared_state.get_rmse_T.remote())\n",
    "        \n",
    "        sorted_EO_S_XGB = sorted(EXPECTED_S_XGB_total.keys())\n",
    "        sorted_EO_S_XGB = [EXPECTED_S_XGB_total[key] for key in sorted_EO_S_XGB]\n",
    "        sorted_EO_T_XGB = sorted(EXPECTED_T_XGB_total.keys())\n",
    "        sorted_EO_T_XGB = [EXPECTED_T_XGB_total[key] for key in sorted_EO_T_XGB]\n",
    "        \n",
    "        sorted_keys_TRUE = sorted(EXPECTED_TRUE.keys())\n",
    "        sorted_keys_TRUE = [EXPECTED_TRUE[key] for key in sorted_keys_TRUE]\n",
    "        \n",
    "        sorted_keys_random = sorted(EXPECTED_random.keys())\n",
    "        sorted_keys_random = [EXPECTED_random[key] for key in sorted_keys_random]\n",
    "        \n",
    "        sorted_keys_optimal = sorted(EXPECTED_optimal.keys())\n",
    "        sorted_keys_optimal = [EXPECTED_optimal[key] for key in sorted_keys_optimal]\n",
    "        \n",
    "        sorted_keys_zero = sorted(EXPECTED_zero.keys())\n",
    "        sorted_keys_zero = [EXPECTED_zero[key] for key in sorted_keys_zero]\n",
    "       \n",
    "        \n",
    "        pourcentages = list(range(0, 101, 5))\n",
    "               \n",
    "        sorted_keys_S_XGB = sorted(rmse_S.keys())\n",
    "        sorted_keys_S_XGB = [rmse_S[key] for key in sorted_keys_S_XGB]\n",
    "        \n",
    "        sorted_keys_T_XGB = sorted(rmse_T.keys())\n",
    "        sorted_keys_T_XGB = [rmse_T[key] for key in sorted_keys_T_XGB]\n",
    "        \n",
    "        pourcentages = list(range(0, 101, 5))\n",
    "               \n",
    "        results_folder = 'Results_2_R_DR'  # ou 'chemin/vers/le/dossier/Results'\n",
    "        \n",
    "        if not os.path.exists(results_folder):\n",
    "            os.makedirs(results_folder)\n",
    "        \n",
    "        parts = filename.split('_')\n",
    "        nb_t = parts[0]  # Le nombre avant le tiret\n",
    "        suffix = parts[1] if len(parts) > 1 else \"\"  # Le suffixe après le tiret\n",
    "        \n",
    "        if suffix.startswith('01'):\n",
    "            condition = 'commence par 01'\n",
    "            data_uplift='01'\n",
    "            fold = filename.split(\"01\", 1)[1]\n",
    "            fold = fold.rstrip('.csv')\n",
    "        elif suffix.startswith('0406'):\n",
    "            condition = 'commence par 0406'\n",
    "            data_uplift='0406'\n",
    "            fold = filename.split(\"0406\", 1)[1]\n",
    "            fold = fold.rstrip('.csv')\n",
    "        elif suffix.startswith('0208'):\n",
    "            condition = 'commence par 0208'\n",
    "            data_uplift='0208'\n",
    "            fold = filename.split(\"0208\", 1)[1]\n",
    "            fold = fold.rstrip('.csv')\n",
    "        else:\n",
    "            condition = 'ne correspond à aucune condition'\n",
    "        \n",
    "        df = pd.DataFrame(columns=[\"Methode\", \"Data uplift\", \"Nb ind par t\",\"Nb de t\",\"Nb case uplift\",\"Fold\",\"EO test\",\"EO test list\",\n",
    "                                   \"RMSE test\",\"RMSE test list\"])\n",
    "        \n",
    "        new_row = {\n",
    "            \"Methode\": \"R Learner XGB\",\n",
    "            \"Data uplift\": data_uplift,  \n",
    "            \"Nb ind par t\": 5000,  \n",
    "            \"Nb de t\": nb_t,  \n",
    "            \"Nb case uplift\": 20,  \n",
    "            \"Fold\" : fold,\n",
    "            \"EO test\":sum(sorted_EO_S_XGB) / len(sorted_EO_S_XGB),\n",
    "            \"EO test list\":sorted_EO_S_XGB,\n",
    "            \"RMSE test\":sum(sorted_keys_S_XGB) / len(sorted_keys_S_XGB),\n",
    "            \"RMSE test list\": sorted_keys_S_XGB\n",
    "        \n",
    "        }\n",
    "        \n",
    "        new_row = pd.DataFrame([new_row]) \n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        \n",
    "        new_row = {\n",
    "            \"Methode\": \"DR Learner XGB\",\n",
    "            \"Data uplift\": data_uplift,  \n",
    "            \"Nb ind par t\": 5000,  \n",
    "            \"Nb de t\": nb_t,  \n",
    "            \"Nb case uplift\": 20,  \n",
    "            \"Fold\" : fold,\n",
    "            \"EO test\":sum(sorted_EO_T_XGB) / len(sorted_EO_T_XGB),\n",
    "            \"EO test list\":sorted_EO_T_XGB,\n",
    "            \"RMSE test\":sum(sorted_keys_T_XGB) / len(sorted_keys_T_XGB),\n",
    "            \"RMSE test list\": sorted_keys_T_XGB\n",
    "        \n",
    "        }\n",
    "        \n",
    "        new_row = pd.DataFrame([new_row]) \n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        titre=nb_t+'_' + data_uplift+fold+'.csv'\n",
    "        csv_path = os.path.join(results_folder, titre)\n",
    "        df.to_csv(csv_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0d9aa-f30f-4e9a-ab84-5c6742e519a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864f461-c0d7-4de6-be9b-b69e639302d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664d9a5-2d9d-44f8-846b-9aa59f1cfe2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd184b-ff7d-4048-a0bb-99e583dec7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e880a69-a505-4784-9251-e91a5861df95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
