{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27a5426-79b7-4a53-aad5-0d0840e7aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from causalml.dataset import synthetic_data\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "#from causalml.inference.meta import BaseSClassifier, BaseTClassifier, BaseXClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.base import clone\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "from sklift.metrics import qini_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn import preprocessing\n",
    "import kuplift as kp\n",
    "import warnings\n",
    "import importlib\n",
    "import subprocess\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import ray\n",
    "from modèles import *\n",
    "from évaluation import *\n",
    "from generate_synth import *\n",
    "import logging\n",
    "import gc\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "from causalml.dataset import synthetic_data\n",
    "import matplotlib\n",
    "from causalml.inference.meta import BaseSClassifier, BaseTClassifier, BaseXClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import causalml\n",
    "from causalml.inference.tree import CausalTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21f44fe-a4a6-4f36-9a99-71364905da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dc82b-b0e4-4a1f-b7df-a66f52688e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   0%|          | 0/150 [00:00<?, ?fichier/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_0110.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 00:10:56,420\tINFO worker.py:1770 -- Started a local Ray instance.\n",
      "\u001b[36m(pid=1004622)\u001b[0m Failed to import duecredit due to No module named 'duecredit'\n",
      "Traitement des fichiers CSV:   1%|▏         | 2/150 [01:36<1:58:44, 48.14s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_04068.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   2%|▏         | 3/150 [08:56<8:38:35, 211.67s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   3%|▎         | 4/150 [16:33<12:17:34, 303.11s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_04067.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   3%|▎         | 5/150 [18:56<9:58:05, 247.48s/fichier] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_04061.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   4%|▍         | 6/150 [21:31<8:40:21, 216.81s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_04065.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   5%|▍         | 7/150 [22:18<6:26:22, 162.12s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17_02086.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   5%|▌         | 8/150 [26:10<7:15:42, 184.10s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_04069.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   6%|▌         | 9/150 [33:29<10:18:06, 263.02s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   7%|▋         | 10/150 [40:50<12:21:52, 317.95s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_04062.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   7%|▋         | 11/150 [48:02<13:36:59, 352.66s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17_02085.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   8%|▊         | 12/150 [51:40<11:56:45, 311.64s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_020810.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   9%|▊         | 13/150 [58:50<13:13:40, 347.60s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_04063.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:   9%|▉         | 14/150 [1:06:05<14:07:47, 374.03s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_04068.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  10%|█         | 15/150 [1:08:26<11:23:11, 303.64s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_02087.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  11%|█         | 16/150 [1:09:10<8:23:42, 225.54s/fichier] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_018.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  11%|█▏        | 17/150 [1:10:36<6:47:26, 183.81s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_04062.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  12%|█▏        | 18/150 [1:13:01<6:18:38, 172.11s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_0110.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  13%|█▎        | 19/150 [1:15:23<5:56:00, 163.06s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17_02081.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  13%|█▎        | 20/150 [1:19:01<6:28:57, 179.52s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_020810.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  14%|█▍        | 21/150 [1:19:49<5:00:45, 139.88s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17_04062.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  15%|█▍        | 22/150 [1:23:36<5:54:16, 166.07s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  15%|█▌        | 23/150 [1:24:22<4:35:10, 130.00s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_02084.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  16%|█▌        | 24/150 [1:26:43<4:40:11, 133.42s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_04067.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  17%|█▋        | 25/150 [1:33:51<7:42:15, 221.89s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_02084.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  17%|█▋        | 26/150 [1:34:37<5:49:21, 169.04s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  18%|█▊        | 27/150 [1:36:03<4:55:17, 144.05s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_019.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  19%|█▊        | 28/150 [1:43:27<7:56:11, 234.19s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_02088.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers CSV:  19%|█▉        | 29/150 [1:51:07<10:08:35, 301.78s/fichier]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33_012.csv\n"
     ]
    }
   ],
   "source": [
    "dict_rmse_1={'S_learner_XGB' : [], 'T_learner_XGB' : [], 'Baseline average' : [], 'Politique optimale' : []}\n",
    "\n",
    "dict_rmse_2={'S_learner_XGB' : [], 'T_learner_XGB' : [], 'Baseline average' : [], 'Politique optimale' : []}\n",
    "\n",
    "dict_expected={'S_learner_XGB' : [],'T_learner_XGB' : [], 'Baseline average' : [], 'Politique optimale' : []}\n",
    "\n",
    "EXPECTED_S_XGB= {}\n",
    "\n",
    "EXPECTED_T_XGB= {}\n",
    "\n",
    "def get_max_index(array):\n",
    "    # Crée un tableau pour stocker les indices des valeurs maximales\n",
    "\n",
    "    try:\n",
    "        array = array.astype(float)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Le tableau contient des valeurs non numériques qui ne peuvent pas être converties.\")\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    # Parcourir chaque individu (chaque ligne du tableau)\n",
    "    for row in array:\n",
    "        # Vérifie si toutes les valeurs de la ligne sont inférieures ou égales à zéro\n",
    "        if np.all(row <= 0):\n",
    "            result.append(0)  # Si aucune valeur n'est supérieure à 0, ajouter 0\n",
    "        else:\n",
    "            # Sinon, trouve l'indice de la valeur maximale dans la ligne\n",
    "            max_index = np.argmax(row)\n",
    "            result.append(max_index + 1)  # Ajouter +1 pour ne pas avoir 0 comme index\n",
    "            \n",
    "    return np.array(result)\n",
    "\n",
    "@ray.remote\n",
    "class ResultActor:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def update(self, percent, value):\n",
    "        self.data[percent] = value\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class SharedState:\n",
    "    def __init__(self):\n",
    "        self.expected_s_xgb = {}\n",
    "        self.expected_t_xgb = {}\n",
    "        self.EXPECTED_TRUE= {}\n",
    "        self.EXPECTED_random= {}\n",
    "        self.EXPECTED_optimal= {}\n",
    "        self.EXPECTED_zero= {}\n",
    "        self.EXPECTED_average= {}\n",
    "        self.nb_bons_t_S= {}\n",
    "        self.nb_bons_t_T= {}\n",
    "        self.nb_bons_t_true= {}\n",
    "        self.nb_bons_t_random= {}\n",
    "        self.nb_bons_t_average= {}\n",
    "        self.rmse_1_S = {}\n",
    "        self.rmse_1_T = {}\n",
    "        \n",
    "    def update(self, percent, s_value, t_value,expected_true,expected_random,expected_optimal,expected_zero,rmse_1_S,rmse_1_T):\n",
    "        self.expected_s_xgb[percent] = s_value\n",
    "        self.expected_t_xgb[percent] = t_value\n",
    "        self.EXPECTED_TRUE[percent] = expected_true\n",
    "        self.EXPECTED_random[percent] = expected_random\n",
    "        self.EXPECTED_optimal[percent] = expected_optimal\n",
    "        self.EXPECTED_zero[percent] = expected_zero\n",
    "        #self.EXPECTED_average[percent] = expected_average\n",
    "        #self.nb_bons_t_S[percent] = nb_bons_t_S\n",
    "        #self.nb_bons_t_T[percent] = nb_bons_t_T\n",
    "        #self.nb_bons_t_true[percent] = nb_bons_t_true\n",
    "        #self.nb_bons_t_random[percent] = nb_bons_t_random\n",
    "        #self.nb_bons_t_average[percent] = nb_bons_t_average\n",
    "        self.rmse_1_S[percent] = rmse_1_S\n",
    "        self.rmse_1_T[percent] = rmse_1_T\n",
    "\n",
    "    def get_expected_s_xgb(self):\n",
    "        return self.expected_s_xgb\n",
    "\n",
    "    def get_expected_t_xgb(self):\n",
    "        return self.expected_t_xgb\n",
    "\n",
    "    def get_expected_true(self):\n",
    "        return self.EXPECTED_TRUE\n",
    "\n",
    "    def get_expected_random(self):\n",
    "        return self.EXPECTED_random\n",
    "\n",
    "    def get_expected_opti(self):\n",
    "        return self.EXPECTED_optimal\n",
    "\n",
    "    def get_expected_zero(self):\n",
    "        return self.EXPECTED_zero\n",
    "\n",
    "    def get_expected_average(self):\n",
    "        return self.EXPECTED_average\n",
    "\n",
    "    def get_nb_bon_t_S(self):\n",
    "        return self.nb_bons_t_S\n",
    "\n",
    "    def get_nb_bon_t_T(self):\n",
    "        return self.nb_bons_t_T\n",
    "\n",
    "    def get_nb_bon_t_true(self):\n",
    "        return self.nb_bons_t_true\n",
    "\n",
    "    def get_nb_bon_t_random(self):\n",
    "        return self.nb_bons_t_random\n",
    "\n",
    "    def get_nb_bon_t_average(self):\n",
    "        return self.nb_bons_t_average\n",
    "        \n",
    "    def get_rmse_S(self):\n",
    "        return self.rmse_1_S\n",
    "\n",
    "    def get_rmse_T(self):\n",
    "        return self.rmse_1_T\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def iteration(data,percent,shared_state_actor,decoupage=4,nb_per_t=1000,NRA='NRA',valeur_min_int=1,valeur_max_int = 4,nb_inter = 8,X1='X1',X2='X2'):\n",
    "    #data = ray.get(data_ref)\n",
    "\n",
    "    start_time = time.time()\n",
    "   \n",
    "    nombre_valeurs_uniques_T = data['T'].nunique()\n",
    "    df_grand = pd.DataFrame()\n",
    "    seed=42\n",
    "    random.seed(seed)\n",
    "    for i in range(nombre_valeurs_uniques_T):\n",
    "        motif_random = generer_motif_random(valeur_min_int, valeur_max_int, nb_inter)\n",
    "        motif = motif_to_int(motif_random,decoupage=decoupage)\n",
    "        dataset_T = data[data['T'] == i]\n",
    "        #dataset_T[NRA] = 0\n",
    "        dataset_T.loc[:, NRA] = 0\n",
    "        for index, row in dataset_T.iterrows():\n",
    "            for i in motif:\n",
    "                if (i[0][0] <= row[X1] <= i[0][1]) & (i[1][0] <= row[X2] <= i[1][1]):\n",
    "                    #dataset_T.at[index, NRA] = 1\n",
    "                    dataset_T.loc[index, NRA] = 1\n",
    "        data1 = dataset_T.head(nb_per_t)\n",
    "        data1 = drop_nra(data1,k=percent,NRA='NRA')\n",
    "        data2 = dataset_T.iloc[nb_per_t:]\n",
    "        data_NRA_0 = data2[data2['NRA'] == 0]\n",
    "        data_remplace = data_NRA_0.head(int(nb_per_t - len(data1)))\n",
    "        data_percent_NRA = pd.concat([data1, data_remplace], ignore_index=True)\n",
    "        df_grand = pd.concat([df_grand, data_percent_NRA], ignore_index=True)\n",
    "    \n",
    "    del data1, data2, data_NRA_0, data_remplace, data_percent_NRA\n",
    "    gc.collect()\n",
    "\n",
    "    valeurs_unique = df_grand['T'].unique()\n",
    "    \n",
    "    colonnes_a_conserver = ['X1', 'X2', 'T']\n",
    "    X_train = df_grand[colonnes_a_conserver]\n",
    "    colonnes_a_conserver = ['X1', 'X2', 'T', 'Y']\n",
    "    X_train_t = df_grand[colonnes_a_conserver]\n",
    "    colonnes_a_conserver = ['X1', 'X2', 'T']\n",
    "    X_test = data[colonnes_a_conserver]\n",
    "    X_test = X_test.drop(X_train.index)\n",
    "    colonnes_a_conserver = ['Y']\n",
    "    y_train = df_grand[colonnes_a_conserver]\n",
    "    #train_final_filtered = df_grand.loc[:, ['X1', 'X2', 'T', 'Y']]\n",
    "    colonnes_a_conserver = ['X1', 'X2', 'T', 'Y']\n",
    "    X_test_t = data[colonnes_a_conserver]\n",
    "    X_test_t = X_test_t.drop(X_train.index)\n",
    "    test_all = data.copy()\n",
    "    test_all = test_all.drop(X_train.index)\n",
    "    random_uplift = expected_to_uplift(random_expected_outcome(X_test_t, 'T'))\n",
    "    random_policy = uplift_to_policy(random_uplift)\n",
    "    \n",
    "\n",
    "    \n",
    "    X_train = df_grand[['X1', 'X2']]\n",
    "    treatment_train = df_grand['T']\n",
    "    y_train = df_grand['Y']\n",
    "\n",
    "    X_test = data[['X1', 'X2']]\n",
    "    X_test = X_test.drop(X_train.index)\n",
    "    treatment_test = data['T']\n",
    "    treatment_test = treatment_test.drop(X_train.index)\n",
    "    y_test = data['Y']\n",
    "    y_test = y_test.drop(X_train.index)\n",
    "   \n",
    "    data_test_t=data.copy()\n",
    "    column_names = data_test_t.columns.tolist()\n",
    "    data_test_t['policy'] = data_test_t.apply(determine_value, axis=1)\n",
    "    tau_optimal_dict = {}\n",
    "    #colonnes_E_y = [col for col in RMSE_test.columns if col.startswith('E_y|T') and not col.endswith('no_rand')]\n",
    "\n",
    "    #optimal_policy = colonnes_max.apply(lambda x: int(x.split('_')[1]))\n",
    "    optimal_policy = data_test_t['policy']\n",
    "    data_test_t['worst_policy'] = data_test_t.apply(determine_value_2, axis=1)\n",
    "    worst_policy = data_test_t['worst_policy'].to_numpy()\n",
    "    zero_policy=[0]*len(random_policy)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    treatment_train = treatment_train.astype(str)\n",
    "    \n",
    "    #CTS\n",
    "    \n",
    "    cts = UpliftRandomForestClassifier(control_name='0', n_estimators=70,max_depth=70,random_state=42,evaluationFunction = 'CTS')\n",
    "\n",
    "    cts.fit(X=X_train.values, treatment=treatment_train.values, y=y_train.values)\n",
    "\n",
    "\n",
    "    uplift_S_learner_XGB = cts.predict(X=X_test.values)\n",
    "    predict_policy_S_learner_XGB = get_max_index(uplift_S_learner_XGB)\n",
    "\n",
    "    #predict_worst_policy_X_learner_XGB = s_learner_model_XGB .predict_worst_policy(X_test)\n",
    "    \n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    start_time = time.time()\n",
    "   \n",
    "    #X learner (RandomForest)\n",
    "\n",
    "\n",
    "    uplift_T_learner_XGB = uplift_S_learner_XGB\n",
    "    predict_policy_T_learner_XGB = predict_policy_S_learner_XGB\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    expected_outcome_S_learner_XGB=expected_outcome(X_test_t,'T','Y',policy=predict_policy_S_learner_XGB)\n",
    "    expected_outcome_T_learner_XGB=expected_outcome(X_test_t,'T','Y',policy=predict_policy_T_learner_XGB)\n",
    "    expected_outcome_true_policy=expected_outcome(X=X_test_t,traitement = 'T',outcome = 'Y')\n",
    "    expected_outcome_random_policy=expected_outcome(X=X_test_t,traitement = 'T',outcome = 'Y',policy=random_policy)\n",
    "    expected_outcome_optimal=expected_outcome(X=X_test_t,traitement = 'T',outcome = 'Y',policy=optimal_policy)\n",
    "    expected_outcome_zero_policy=expected_outcome(X_test_t,'T','Y',policy=zero_policy)\n",
    "\n",
    "    politiques = ['S-Learner_XGB'\n",
    "                'T-Learner_XGB', \n",
    "                'True Policy', 'Random Policy', 'Optimal Policy','Zero policy', 'S_Learner MLP2','Baseline average']\n",
    "\n",
    "    values = [expected_outcome_S_learner_XGB, \n",
    "             expected_outcome_T_learner_XGB,  \n",
    "            expected_outcome_true_policy, expected_outcome_random_policy,expected_outcome_optimal,expected_outcome_zero_policy]\n",
    "\n",
    "    politiques, values = zip(*sorted(zip(politiques, values), key=lambda x: x[1], reverse=False))\n",
    "\n",
    "    bar_width = 0.5\n",
    "    bar_positions = np.arange(len(politiques))\n",
    "    brilliant_color = (1.0, 0.8, 0.0)\n",
    "\n",
    "    tau_1_0 = test_all['tau_1_0'].values\n",
    "    tau_2_0 = test_all['tau_2_0'].values\n",
    "\n",
    "    tau_columns = [col for col in test_all.columns if col.startswith('tau_')]\n",
    "    num_tau_columns = len(tau_columns)\n",
    "    rmse_list = []\n",
    "    for i, tau_col in enumerate(tau_columns):\n",
    "        true_values = test_all[tau_col].values\n",
    "        \n",
    "        pred_value = [sublist[i] for sublist in uplift_S_learner_XGB]\n",
    "        rmse = calculate_rmse(true_values, pred_value)\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "    # Calculer la RMSE moyenne\n",
    "    average_rmse = np.mean(rmse_list)\n",
    "    tau_1_0_S_learner_XGB = [item[0] for item in uplift_S_learner_XGB]\n",
    "    tau_2_0_S_learner_XGB = [item[1] for item in uplift_S_learner_XGB]\n",
    "    rmse_tau_1_0_S_learner_XGB = average_rmse\n",
    "    rmse_tau_2_0_S_learner_XGB = np.sqrt(mean_squared_error(tau_2_0, tau_2_0_S_learner_XGB))\n",
    "\n",
    "    rmse_list = []\n",
    "    for i, tau_col in enumerate(tau_columns):\n",
    "        true_values = test_all[tau_col].values\n",
    "        \n",
    "        pred_value = [sublist[i] for sublist in uplift_T_learner_XGB]\n",
    "        rmse = calculate_rmse(true_values, pred_value)\n",
    "        rmse_list.append(rmse)\n",
    "    average_rmse = np.mean(rmse_list)\n",
    "\n",
    "    tau_1_0_T_learner_XGB = [item[0] for item in uplift_T_learner_XGB]\n",
    "    tau_2_0_T_learner_XGB = [item[1] for item in uplift_T_learner_XGB]\n",
    "    rmse_tau_1_0_T_learner_XGB = average_rmse\n",
    "    rmse_tau_2_0_T_learner_XGB = np.sqrt(mean_squared_error(tau_2_0, tau_2_0_T_learner_XGB))\n",
    "\n",
    "    tau_1_0_optimal = test_all.apply(lambda row: row['E_y|T1_no_rand'] - row['E_y|T0_no_rand'], axis=1)\n",
    "    tau_2_0_optimal = test_all.apply(lambda row: row['E_y|T2_no_rand'] - row['E_y|T0_no_rand'], axis=1)\n",
    "    uplift_optimal = [[diff, value] for diff, value in zip(tau_1_0_optimal, tau_2_0_optimal)]\n",
    "    rmse_tau_1_0_optimal = np.sqrt(mean_squared_error(tau_1_0, tau_1_0_optimal))\n",
    "    rmse_tau_2_0_optimal = np.sqrt(mean_squared_error(tau_2_0, tau_2_0_optimal))\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    \n",
    "    methods = ['S_learner_XGB', 'T_learner_XGB', 'Optimal','Baseline average']\n",
    "\n",
    "    rmse_tau_1_0_list = [rmse_tau_1_0_S_learner_XGB, rmse_tau_1_0_T_learner_XGB, rmse_tau_1_0_optimal]\n",
    "\n",
    "    rmse_tau_2_0_list = [rmse_tau_2_0_S_learner_XGB, rmse_tau_2_0_T_learner_XGB, rmse_tau_2_0_optimal]\n",
    "\n",
    "    pairs_tau_1_0 = zip(methods, rmse_tau_1_0_list)\n",
    "    pairs_tau_2_0 = zip(methods, rmse_tau_2_0_list)\n",
    "    sorted_pairs_tau_1_0 = sorted(pairs_tau_1_0, key=lambda x: x[1])\n",
    "    sorted_pairs_tau_2_0 = sorted(pairs_tau_2_0, key=lambda x: x[1])\n",
    "    sorted_methods_tau_1_0, sorted_rmse_tau_1_0 = zip(*sorted_pairs_tau_1_0)\n",
    "    sorted_methods_tau_2_0, sorted_rmse_tau_2_0 = zip(*sorted_pairs_tau_2_0)\n",
    "    OPTI = optimal_policy[:1000]\n",
    "    \n",
    "    optimal_policy_df = pd.DataFrame(OPTI, columns=['policy'])\n",
    "    optimal_policy_random = optimal_policy_df.sample(n=100, random_state=42)\n",
    "\n",
    "    true_expected_outcome_S_learner_XGB = true_expected_outcome(test_all,predict_policy_S_learner_XGB)\n",
    "    true_expected_outcome_T_learner_XGB = true_expected_outcome(test_all,predict_policy_T_learner_XGB)\n",
    "    true_expected_outcome_optimal = true_expected_outcome(test_all,optimal_policy)\n",
    "    \n",
    "    dict_rmse_1['S_learner_XGB'].append(rmse_tau_1_0_S_learner_XGB)\n",
    "    dict_rmse_1['T_learner_XGB'].append(rmse_tau_1_0_T_learner_XGB)\n",
    "    dict_rmse_1['Politique optimale'].append(rmse_tau_1_0_optimal)\n",
    "    dict_rmse_2['S_learner_XGB'].append(rmse_tau_2_0_S_learner_XGB)\n",
    "    dict_rmse_2['T_learner_XGB'].append(rmse_tau_2_0_T_learner_XGB)\n",
    "    dict_rmse_2['Politique optimale'].append(rmse_tau_2_0_optimal)\n",
    "    dict_expected['S_learner_XGB'].append(expected_outcome_S_learner_XGB)\n",
    "    dict_expected['T_learner_XGB'].append(expected_outcome_T_learner_XGB)\n",
    "    dict_expected['Politique optimale'].append(expected_outcome_optimal)\n",
    "    X1 = X_train_t[X_train_t['T'] == 1]['X1']\n",
    "    X2 = X_train_t[X_train_t['T'] == 1]['X2']\n",
    "    E_y_T0 = X_train_t[X_train_t['T'] == 1]['Y']\n",
    "    EXPECTED_S_XGB = {percent: true_expected_outcome_S_learner_XGB}\n",
    "    EXPECTED_T_XGB = {percent: true_expected_outcome_T_learner_XGB}\n",
    "    EXPECTED_TRUE = {percent: expected_outcome_true_policy}\n",
    "    EXPECTED_random = {percent: expected_outcome_random_policy}\n",
    "    EXPECTED_optimal = {percent: true_expected_outcome_optimal}\n",
    "    EXPECTED_zero = {percent: expected_outcome_zero_policy}\n",
    "    \n",
    "    rmse_S = {percent: rmse_tau_1_0_S_learner_XGB}\n",
    "    rmse_T = {percent: rmse_tau_1_0_T_learner_XGB}\n",
    "    \n",
    "    shared_state_actor.update.remote(percent, EXPECTED_S_XGB[percent], EXPECTED_T_XGB[percent],EXPECTED_TRUE[percent],EXPECTED_random[percent],EXPECTED_optimal[percent],\n",
    "                                     EXPECTED_zero[percent],rmse_S[percent],rmse_T[percent])\n",
    "    ray.get(result_actor.update.remote(percent, expected_outcome_S_learner_XGB))\n",
    "    return expected_outcome_S_learner_XGB, expected_outcome_T_learner_XGB\n",
    "\n",
    "exclusions = ['33_016.csv']\n",
    "\n",
    "dataset_directory = \"/data/userstorage/nleboudec/Datasets_générés\"\n",
    "\n",
    "for filename in tqdm(os.listdir(dataset_directory), desc=\"Traitement des fichiers CSV\", unit=\"fichier\"):\n",
    "    if filename.endswith(\".csv\") and filename not in exclusions:\n",
    "    #if filename.endswith(\".csv\"):  # Vérifie si le fichier est un fichier CSV\n",
    "        print(filename)\n",
    "        result_actor = ResultActor.remote()\n",
    "        shared_state = SharedState.remote()\n",
    "        file_path = os.path.join(dataset_directory, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        percentages = list(range(0, 101, 5))\n",
    "        futures = [iteration.remote(data,i,shared_state,nb_per_t=5000) for i in percentages]\n",
    "        results = ray.get(futures)\n",
    "        DICT8_EXPECTED = ray.get(result_actor.get_data.remote())\n",
    "        EXPECTED_S_XGB_total = ray.get(shared_state.get_expected_s_xgb.remote())\n",
    "        EXPECTED_T_XGB_total = ray.get(shared_state.get_expected_t_xgb.remote())\n",
    "        EXPECTED_TRUE = ray.get(shared_state.get_expected_true.remote())\n",
    "        EXPECTED_random = ray.get(shared_state.get_expected_random.remote())\n",
    "        EXPECTED_optimal = ray.get(shared_state.get_expected_opti.remote())\n",
    "        EXPECTED_zero = ray.get(shared_state.get_expected_zero.remote())\n",
    "        \n",
    "        \n",
    "        rmse_S = ray.get(shared_state.get_rmse_S.remote())\n",
    "        rmse_T = ray.get(shared_state.get_rmse_T.remote())\n",
    "        \n",
    "        sorted_EO_S_XGB = sorted(EXPECTED_S_XGB_total.keys())\n",
    "        sorted_EO_S_XGB = [EXPECTED_S_XGB_total[key] for key in sorted_EO_S_XGB]\n",
    "        sorted_EO_T_XGB = sorted(EXPECTED_T_XGB_total.keys())\n",
    "        sorted_EO_T_XGB = [EXPECTED_T_XGB_total[key] for key in sorted_EO_T_XGB]\n",
    "        \n",
    "        sorted_keys_TRUE = sorted(EXPECTED_TRUE.keys())\n",
    "        sorted_keys_TRUE = [EXPECTED_TRUE[key] for key in sorted_keys_TRUE]\n",
    "        \n",
    "        sorted_keys_random = sorted(EXPECTED_random.keys())\n",
    "        sorted_keys_random = [EXPECTED_random[key] for key in sorted_keys_random]\n",
    "        \n",
    "        sorted_keys_optimal = sorted(EXPECTED_optimal.keys())\n",
    "        sorted_keys_optimal = [EXPECTED_optimal[key] for key in sorted_keys_optimal]\n",
    "        \n",
    "        sorted_keys_zero = sorted(EXPECTED_zero.keys())\n",
    "        sorted_keys_zero = [EXPECTED_zero[key] for key in sorted_keys_zero]\n",
    "       \n",
    "        \n",
    "        pourcentages = list(range(0, 101, 5))\n",
    "               \n",
    "        sorted_keys_S_XGB = sorted(rmse_S.keys())\n",
    "        sorted_keys_S_XGB = [rmse_S[key] for key in sorted_keys_S_XGB]\n",
    "        \n",
    "        sorted_keys_T_XGB = sorted(rmse_T.keys())\n",
    "        sorted_keys_T_XGB = [rmse_T[key] for key in sorted_keys_T_XGB]\n",
    "        \n",
    "        pourcentages = list(range(0, 101, 5))\n",
    "               \n",
    "        results_folder = 'Results_CTS'  # ou 'chemin/vers/le/dossier/Results'\n",
    "        \n",
    "        if not os.path.exists(results_folder):\n",
    "            os.makedirs(results_folder)\n",
    "        \n",
    "        parts = filename.split('_')\n",
    "        nb_t = parts[0]  # Le nombre avant le tiret\n",
    "        suffix = parts[1] if len(parts) > 1 else \"\"  # Le suffixe après le tiret\n",
    "        \n",
    "        if suffix.startswith('01'):\n",
    "            condition = 'commence par 01'\n",
    "            data_uplift='01'\n",
    "            fold = filename.split(\"01\", 1)[1]\n",
    "            fold = fold.rstrip('.csv')\n",
    "        elif suffix.startswith('0406'):\n",
    "            condition = 'commence par 0406'\n",
    "            data_uplift='0406'\n",
    "            fold = filename.split(\"0406\", 1)[1]\n",
    "            fold = fold.rstrip('.csv')\n",
    "        elif suffix.startswith('0208'):\n",
    "            condition = 'commence par 0208'\n",
    "            data_uplift='0208'\n",
    "            fold = filename.split(\"0208\", 1)[1]\n",
    "            fold = fold.rstrip('.csv')\n",
    "        else:\n",
    "            condition = 'ne correspond à aucune condition'\n",
    "        \n",
    "        df = pd.DataFrame(columns=[\"Methode\", \"Data uplift\", \"Nb ind par t\",\"Nb de t\",\"Nb case uplift\",\"Fold\",\"EO test\",\"EO test list\",\n",
    "                                   \"RMSE test\",\"RMSE test list\"])\n",
    "        \n",
    "        new_row = {\n",
    "            \"Methode\": \"CTS\",\n",
    "            \"Data uplift\": data_uplift,  \n",
    "            \"Nb ind par t\": 5000,  \n",
    "            \"Nb de t\": nb_t,  \n",
    "            \"Nb case uplift\": 20,  \n",
    "            \"Fold\" : fold,\n",
    "            \"EO test\":sum(sorted_EO_S_XGB) / len(sorted_EO_S_XGB),\n",
    "            \"EO test list\":sorted_EO_S_XGB,\n",
    "            \"RMSE test\":sum(sorted_keys_S_XGB) / len(sorted_keys_S_XGB),\n",
    "            \"RMSE test list\": sorted_keys_S_XGB\n",
    "        \n",
    "        }\n",
    "        \n",
    "        new_row = pd.DataFrame([new_row]) \n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        \n",
    "        new_row = {\n",
    "            \"Methode\": \"X Learner LR\",\n",
    "            \"Data uplift\": data_uplift,  \n",
    "            \"Nb ind par t\": 5000,  \n",
    "            \"Nb de t\": nb_t,  \n",
    "            \"Nb case uplift\": 20,  \n",
    "            \"Fold\" : fold,\n",
    "            \"EO test\":sum(sorted_EO_T_XGB) / len(sorted_EO_T_XGB),\n",
    "            \"EO test list\":sorted_EO_T_XGB,\n",
    "            \"RMSE test\":sum(sorted_keys_T_XGB) / len(sorted_keys_T_XGB),\n",
    "            \"RMSE test list\": sorted_keys_T_XGB\n",
    "        \n",
    "        }\n",
    "        \n",
    "        new_row = pd.DataFrame([new_row]) \n",
    "        #df = pd.concat([df, new_row], ignore_index=True)\n",
    "        titre=nb_t+'_' + data_uplift+fold+'.csv'\n",
    "        csv_path = os.path.join(results_folder, titre)\n",
    "        df.to_csv(csv_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da0c1e-b5b3-47af-906a-f72df225a471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b61124-6968-42d5-99aa-eb712672edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f01b28-05f5-4126-b3e4-aca431e64004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3eefa-634d-4cfe-ae24-15619f3df298",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d06932-8133-4ccd-a8a6-9f49bb998a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcc4e7-cd72-4fc6-9c58-84de87e8b218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d10a3d-663a-4629-ac60-aeaf89beba37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa0a8b-ca82-471f-8226-3ae3ad1e3b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
